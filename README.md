# ğŸ“Š Streaming Pipeline de Ventes avec Kafka, Spark & Delta Lake

Un pipeline complet de donnÃ©es en temps rÃ©el pour traiter et visualiser des ventes e-commerce. Ce projet dÃ©montre l'architecture Lambda avec ingestion Kafka, traitement Spark Streaming, stockage Delta Lake et dashboard Streamlit.

![Architecture](https://img.shields.io/badge/Architecture-Lambda-blue)
![Python](https://img.shields.io/badge/Python-3.8+-green)
![Kafka](https://img.shields.io/badge/Kafka-Streaming-orange)
![Spark](https://img.shields.io/badge/Spark-3.x-red)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-Storage-purple)

## ğŸ¯ Vue d'ensemble

Ce projet implÃ©mente une architecture de donnÃ©es moderne en trois couches :

- **Bronze Layer** : DonnÃ©es brutes ingÃ©rÃ©es depuis Kafka
- **Silver Layer** : DonnÃ©es nettoyÃ©es et agrÃ©gÃ©es
- **Gold Layer** : Dashboard interactif pour l'analyse business

```
Kafka Producer â†’ Spark Streaming â†’ Delta Lake (Bronze) â†’ Transformation â†’ Delta Lake (Silver) â†’ Streamlit Dashboard
```

## âœ¨ FonctionnalitÃ©s

- âš¡ Ingestion de donnÃ©es en temps rÃ©el via Kafka
- ğŸ”„ Traitement streaming avec Spark Structured Streaming
- ğŸ’¾ Stockage ACID-compliant avec Delta Lake
- ğŸ“Š Dashboard interactif avec mÃ©triques clÃ©s
- ğŸ¨ Visualisations dynamiques (Plotly)
- ğŸŒ Analyse multi-dimensionnelle (clients, produits, pays)

## ğŸ—ï¸ Architecture

```mermaid
graph LR
    A[Producer Kafka] --> B[Topic: ventes_stream]
    B --> C[Spark Streaming]
    C --> D[Delta Lake Bronze]
    D --> E[Transformation Silver]
    E --> F[Delta Lake Silver]
    F --> G[Streamlit Dashboard]
```

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- Apache Kafka 2.8+
- Apache Spark 3.x
- Java 8 ou 11

## ğŸš€ Installation

### 1. Cloner le repository

```bash
git clone https://github.com/votre-username/streaming-ventes-pipeline.git
cd streaming-ventes-pipeline
```

### 2. CrÃ©er un environnement virtuel

```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

**requirements.txt** :
```txt
kafka-python==2.0.2
pyspark==3.4.0
delta-spark==2.4.0
streamlit==1.28.0
pandas==2.0.3
plotly==5.17.0
deltalake==0.12.0
```

### 4. Installer et dÃ©marrer Kafka

**Sur Linux/Mac** :
```bash
# TÃ©lÃ©charger Kafka
wget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz
tar -xzf kafka_2.13-3.6.0.tgz
cd kafka_2.13-3.6.0

# DÃ©marrer Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties &

# DÃ©marrer Kafka
bin/kafka-server-start.sh config/server.properties &
```

**Sur Windows** :
```cmd
# Utiliser les scripts .bat dans le dossier bin\windows\
bin\windows\zookeeper-server-start.bat config\zookeeper.properties
bin\windows\kafka-server-start.bat config\server.properties
```

### 5. CrÃ©er le topic Kafka

```bash
bin/kafka-topics.sh --create \
  --topic ventes_stream \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1
```

## ğŸ® Utilisation

### Ã‰tape 1 : DÃ©marrer le producteur Kafka

```bash
python producer_ventes.py
```

Le producteur gÃ©nÃ¨re des ventes alÃ©atoires toutes les 2 secondes avec :
- 5 produits diffÃ©rents (Ã©lectronique, livres)
- 3 clients types (France, Espagne, UK)
- Segments : Particulier / Entreprise

### Ã‰tape 2 : Lancer le streaming Spark (Bronze Layer)

```bash
python spark_streaming_delta.py
```

Ce script :
- Lit les messages depuis Kafka
- Parse le JSON
- Enrichit avec des timestamps
- Ã‰crit en Delta Lake avec partitionnement par jour

### Ã‰tape 3 : GÃ©nÃ©rer la couche Silver (AgrÃ©gations)

```bash
python streaming_silver.py
```

CrÃ©e des agrÃ©gations par client :
- Total des dÃ©penses
- Nombre d'achats
- Panier moyen
- Indicateur de fidÃ©litÃ©

### Ã‰tape 4 : Visualiser avec Streamlit

```bash
streamlit run dashboard.py
```

Le dashboard affiche :
- ğŸ’° Chiffre d'affaires total
- ğŸ“¦ Volume de ventes
- ğŸ›’ Panier moyen
- ğŸ† Top client
- ğŸ“Š Graphiques interactifs

## ğŸ“ Structure du projet

```
streaming-ventes-pipeline/
â”‚
â”œâ”€â”€ producer_ventes.py          # GÃ©nÃ©rateur de donnÃ©es Kafka
â”œâ”€â”€ spark_streaming_delta.py    # Pipeline streaming Bronze
â”œâ”€â”€ streaming_silver.py         # Transformation Silver
â”œâ”€â”€ dashboard.py                # Dashboard Streamlit
â”‚
â”œâ”€â”€ delta_ventes_table/         # DonnÃ©es Bronze (gÃ©nÃ©rÃ©)
â”œâ”€â”€ delta_silver_aggreges/      # DonnÃ©es Silver (gÃ©nÃ©rÃ©)
â”œâ”€â”€ checkpoint_ventes/          # Checkpoints Spark (gÃ©nÃ©rÃ©)
â”‚
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â””â”€â”€ README.md                   # Ce fichier
```

## ğŸ”§ Configuration

### Variables importantes

**producer_ventes.py** :
```python
KAFKA_TOPIC = "ventes_stream"
KAFKA_SERVER = "localhost:9092"
```

**spark_streaming_delta.py** :
```python
.option("kafka.bootstrap.servers", "localhost:9092")
.option("subscribe", "ventes_stream")
```

**dashboard.py** :
```python
SILVER_PATH = "delta_silver_aggreges"
```

## ğŸ“Š Exemple de donnÃ©es

### Message Kafka (JSON)
```json
{
  "vente_id": 42,
  "client_id": 1,
  "produit_id": 101,
  "timestamp": "2024-12-16T14:30:00",
  "quantite": 2,
  "montant": 1799.98,
  "client_nom": "Jean Dupont",
  "produit_nom": "Ordinateur portable",
  "categorie": "Ã‰lectronique",
  "pays": "France",
  "segment": "Particulier"
}
```

### Table Silver (AgrÃ©gÃ©e)
| client_id | client_nom | pays | total_depense | nb_achats | panier_moyen |
|-----------|------------|------|---------------|-----------|--------------|
| 1 | Jean Dupont | France | 2549.97 | 12 | 212.50 |
| 3 | John Smith | UK | 1899.50 | 8 | 237.44 |

## ğŸ› Troubleshooting

### ProblÃ¨me : "Module 'delta' not found"
```bash
pip install delta-spark==2.4.0
```

### ProblÃ¨me : Kafka ne dÃ©marre pas
VÃ©rifiez que le port 9092 est libre :
```bash
netstat -an | grep 9092
```

### ProblÃ¨me : "delta_ventes_table not found"
Assurez-vous d'avoir lancÃ© `spark_streaming_delta.py` et qu'il ait reÃ§u au moins un message.

### ProblÃ¨me : Encodage UTF-8 sur Windows
Ajoutez en haut des scripts :
```python
# -*- coding: utf-8 -*-
```

## ğŸš€ AmÃ©liorations possibles

- [ ] Ajouter un monitoring avec Prometheus/Grafana
- [ ] ImplÃ©menter des alertes sur seuils (anomalies)
- [ ] CrÃ©er une API REST pour exposer les mÃ©triques
- [ ] Ajouter des tests unitaires (pytest)
- [ ] Containeriser avec Docker Compose
- [ ] ImplÃ©menter du Machine Learning (prÃ©diction de churn)
- [ ] Ajouter une authentification au dashboard

## ğŸ“š Ressources

- [Documentation Kafka](https://kafka.apache.org/documentation/)
- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Delta Lake Documentation](https://docs.delta.io/)
- [Streamlit Docs](https://docs.streamlit.io/)

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit vos changements (`git commit -m 'Ajout fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ‘¨â€ğŸ’» Auteur

**Votre Nom**
- GitHub: [@votre-username](https://github.com/votre-username)
- LinkedIn: [Votre Profil](https://linkedin.com/in/votre-profil)

---

â­ N'oubliez pas de mettre une Ã©toile si ce projet vous a Ã©tÃ© utile !
